{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11fd6df5-4573-421f-9e1a-c97c8a3463ea",
   "metadata": {},
   "source": [
    "### API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da00240f-45fa-4ec6-a9a0-5fb579d36854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e587783d-585b-44ec-a3a7-f928d81f02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "secrets_file_path = os.path.join(\".streamlit\", \"secrets.toml\")\n",
    "if os.path.exists(secrets_file_path):\n",
    "    try:\n",
    "        with open(secrets_file_path,\"r\") as f: \n",
    "            toml_dict = toml.load(f)\n",
    "            os.environ[\"OPENAI_API_KEY\"] = toml_dict[\"OPENAI_API_KEY\"]\n",
    "    except FileNotFoundError:\n",
    "        print('Secrets file not found')\n",
    "else:\n",
    "    print('Secrets file not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a6baa-4006-46a4-8f98-ccb1964bcaba",
   "metadata": {},
   "source": [
    "### Embedding PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23aae26d-54ad-4606-8024-0f2d02bf479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asthapuri/Documents/LLMStreamlitDemoBasic/venv/lib/python3.8/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PagedPDFSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54633d6c-b66b-483c-92e1-e966a85fb1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Astha_Puri_Overall.pdf...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "pdf_directory = \"pdf\"\n",
    "embedding_folder=\"index\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(pdf_directory):\n",
    "    # List all PDF files in the directory\n",
    "    pdf_files = [\n",
    "        file for file in os.listdir(pdf_directory) if file.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    if pdf_files:\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Embedding {pdf_file}...\")\n",
    "            #embed_document(file_name=pdf_file, file_folder=pdf_directory)\n",
    "            file_path = f\"{pdf_directory}/{pdf_file}\"\n",
    "            loader = PagedPDFSplitter(file_path)\n",
    "            source_pages = loader.load_and_split()\n",
    "\n",
    "            embedding_func = OpenAIEmbeddings()\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=500,\n",
    "                chunk_overlap=100,\n",
    "                length_function=len,\n",
    "                is_separator_regex=False,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            )\n",
    "            source_chunks = text_splitter.split_documents(source_pages)\n",
    "            search_index = FAISS.from_documents(source_chunks, embedding_func)\n",
    "            search_index.save_local(\n",
    "                folder_path=embedding_folder, index_name=pdf_file + \".index\"\n",
    "            )\n",
    "            print(\"Done!\")\n",
    "    else:\n",
    "        raise Exception(\"No PDF files found in the directory.\")\n",
    "else:\n",
    "    raise Exception(f\"Directory '{pdf_directory}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e52deaf-725d-4474-b6a1-f804f1a5d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Astha_Puri_Overall.pdf']\n"
     ]
    }
   ],
   "source": [
    "index_directory = \"index\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(index_directory):\n",
    "    # List all index files in the directory\n",
    "    postfix = \".index.faiss\"\n",
    "    index_files = [\n",
    "        file.replace(postfix, \"\")\n",
    "        for file in os.listdir(index_directory)\n",
    "        if file.endswith(postfix)\n",
    "    ]\n",
    "    if index_files:\n",
    "        print(index_files)\n",
    "    else:\n",
    "        raise Exception(\"No index files found in the directory.\")\n",
    "else:\n",
    "    raise Exception(f\"Directory '{index_directory}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa356fe-df00-4876-9172-751c6c3f4b7b",
   "metadata": {},
   "source": [
    "### LLM Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5913bee4-ac40-4a87-9c9b-5fad8db56716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# langchain imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "859780b0-c253-4817-8e5e-504e16fd35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_condense_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {input}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_condense_template)\n",
    "\n",
    "_rag_template = \"\"\"Answer the question based only on the following context, citing the page number(s) of the document(s) you used to answer the question:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca46307-9d3c-4eb8-8ff6-c3f2cb2d1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    res = \"\"\n",
    "    # res = str(docs)\n",
    "    for doc in docs:\n",
    "        escaped_page_content = doc.page_content.replace(\"\\n\", \"\\\\n\")\n",
    "        res += \"<doc>\\n\"\n",
    "        res += f\"  <content>{escaped_page_content}</content>\\n\"\n",
    "        for m in doc.metadata:\n",
    "            res += f\"  <{m}>{doc.metadata[m]}</{m}>\\n\"\n",
    "        res += \"</doc>\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4308d-926d-4519-b3bb-0433f416f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_index(file_name=\"Mahmoudi_Nima_202202_PhD.pdf\", index_folder=\"index\"):\n",
    "    # load embeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    search_index = FAISS.load_local(\n",
    "        folder_path=index_folder,\n",
    "        index_name=file_name + \".index\",\n",
    "        embeddings=OpenAIEmbeddings(),\n",
    "    )\n",
    "    return search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790b3f3-a354-4374-aec4-22cb971d2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_message(m):\n",
    "    if m[\"role\"] == \"user\":\n",
    "        return HumanMessage(content=m[\"content\"])\n",
    "    elif m[\"role\"] == \"assistant\":\n",
    "        return AIMessage(content=m[\"content\"])\n",
    "    elif m[\"role\"] == \"system\":\n",
    "        return SystemMessage(content=m[\"content\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown role {m['role']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf273f7-40da-4f57-b97a-42872ba6601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_chat_history(chat_history):\n",
    "    def format_single_chat_message(m):\n",
    "        if type(m) is HumanMessage:\n",
    "            return \"Human: \" + m.content\n",
    "        elif type(m) is AIMessage:\n",
    "            return \"Assistant: \" + m.content\n",
    "        elif type(m) is SystemMessage:\n",
    "            return \"System: \" + m.content\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role {m['role']}\")\n",
    "\n",
    "    return \"\\n\".join([format_single_chat_message(m) for m in chat_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c1a4b-459e-4bbf-8ead-ead9f4ecac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(file_name=\"Mahmoudi_Nima_202202_PhD.pdf\", index_folder=\"index\", retrieval_cb=None):\n",
    "    vectorstore = get_search_index(file_name, index_folder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    if retrieval_cb is None:\n",
    "        retrieval_cb = lambda x: x\n",
    "\n",
    "    def context_update_fn(q):\n",
    "        retrieval_cb([q])\n",
    "        return q\n",
    "\n",
    "    _inputs = RunnableMap(\n",
    "        standalone_question=RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    )\n",
    "    _context = {\n",
    "        \"context\": itemgetter(\"standalone_question\") | RunnablePassthrough(context_update_fn) | retriever | format_docs,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n",
    "    return conversational_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a49461-2fca-4757-94f5-513890ed462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG fusion chain\n",
    "# source1: https://youtu.be/GchC5WxeXGc?si=6i7J0rPZI7SNwFYZ\n",
    "# source2: https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    from langchain.load import dumps, loads\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4639124-fe68-44b4-9642-8e203979e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_query_generation_chain():\n",
    "    from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "    prompt = ChatPromptTemplate(\n",
    "        input_variables=['original_query'],\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate(\n",
    "                prompt=PromptTemplate(\n",
    "                    input_variables=[],\n",
    "                    template='You are a helpful assistant that generates multiple search queries based on a single input query.'\n",
    "                )\n",
    "            ),\n",
    "            HumanMessagePromptTemplate(\n",
    "                prompt=PromptTemplate(\n",
    "                    input_variables=['original_query'],\n",
    "                    template='Generate multiple search queries related to: {original_query} \\n OUTPUT (4 queries):'\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generate_queries = (\n",
    "        prompt |\n",
    "        ChatOpenAI(temperature=0) |\n",
    "        StrOutputParser() |\n",
    "        (lambda x: x.split(\"\\n\"))\n",
    "    )\n",
    "\n",
    "    return generate_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c984250-3bad-4cc7-95e2-a59764129b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_fusion_chain(file_name=\"Mahmoudi_Nima_202202_PhD.pdf\", index_folder=\"index\", retrieval_cb=None):\n",
    "    vectorstore = get_search_index(file_name, index_folder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    query_generation_chain = get_search_query_generation_chain()\n",
    "    _inputs = RunnableMap(\n",
    "        standalone_question=RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    )\n",
    "\n",
    "    if retrieval_cb is None:\n",
    "        retrieval_cb = lambda x: x\n",
    "\n",
    "    _context = {\n",
    "        \"context\":\n",
    "            RunnablePassthrough.assign(\n",
    "                original_query=lambda x: x[\"standalone_question\"]\n",
    "            )\n",
    "            | query_generation_chain\n",
    "            | retrieval_cb\n",
    "            | retriever.map()\n",
    "            | reciprocal_rank_fusion\n",
    "            | (lambda x: [item[0] for item in x])\n",
    "            | format_docs,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n",
    "    return conversational_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084035dd-7c22-458d-b583-f00ce853bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    question_generation_chain = get_search_query_generation_chain()\n",
    "    print('='*50)\n",
    "    print('RAG Chain')\n",
    "    chain = get_rag_chain()\n",
    "    print(chain.invoke({'input': 'serverless computing', 'chat_history': []}))\n",
    "\n",
    "    print('='*50)\n",
    "    print('Question Generation Chain')\n",
    "    print(question_generation_chain.invoke({'original_query': 'serverless computing'}))\n",
    "\n",
    "    print('-'*50)\n",
    "    print('RAG Fusion Chain')\n",
    "    chain = get_rag_fusion_chain()\n",
    "    print(chain.invoke({'input': 'serverless computing', 'chat_history': []}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091db27-dba2-40b4-a6fd-d58834efd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_method_map = {\n",
    "    'Basic RAG': get_rag_chain,\n",
    "    'RAG Fusion': get_rag_fusion_chain\n",
    "}\n",
    "chosen_rag_method = st.radio(\n",
    "    \"Choose a RAG method\", rag_method_map.keys(), index=0\n",
    ")\n",
    "get_rag_chain_func = rag_method_map[chosen_rag_method]\n",
    "## get the chain WITHOUT the retrieval callback (not used)\n",
    "# custom_chain = get_rag_chain_func(chosen_file)\n",
    "\n",
    "# create the message history state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# render older messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# render the chat input\n",
    "prompt = st.chat_input(\"Enter your message...\")\n",
    "if prompt:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # render the user's new message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # render the assistant's response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        retrival_container = st.container()\n",
    "        message_placeholder = st.empty()\n",
    "\n",
    "        retrieval_status = retrival_container.status(\"**Context Retrieval**\")\n",
    "        queried_questions = []\n",
    "        rendered_questions = set()\n",
    "        def update_retrieval_status():\n",
    "            for q in queried_questions:\n",
    "                if q in rendered_questions:\n",
    "                    continue\n",
    "                rendered_questions.add(q)\n",
    "                retrieval_status.markdown(f\"\\n\\n`- {q}`\")\n",
    "        def retrieval_cb(qs):\n",
    "            for q in qs:\n",
    "                if q not in queried_questions:\n",
    "                    queried_questions.append(q)\n",
    "            return qs\n",
    "        \n",
    "        # get the chain with the retrieval callback\n",
    "        custom_chain = get_rag_chain_func(chosen_file, retrieval_cb=retrieval_cb)\n",
    "        \n",
    "        if \"messages\" in st.session_state:\n",
    "            chat_history = [convert_message(m) for m in st.session_state.messages[:-1]]\n",
    "        else:\n",
    "            chat_history = []\n",
    "\n",
    "        full_response = \"\"\n",
    "        for response in custom_chain.stream(\n",
    "            {\"input\": prompt, \"chat_history\": chat_history}\n",
    "        ):\n",
    "            if \"output\" in response:\n",
    "                full_response += response[\"output\"]\n",
    "            else:\n",
    "                full_response += response.content\n",
    "\n",
    "            message_placeholder.markdown(full_response + \"▌\")\n",
    "            update_retrieval_status()\n",
    "\n",
    "        retrieval_status.update(state=\"complete\")\n",
    "        message_placeholder.markdown(full_response)\n",
    "\n",
    "    # add the full response to the message history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4ea53-9c10-45ea-aaeb-c7ba99a1c322",
   "metadata": {},
   "source": [
    "### Chat - RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0cc551b-c2b6-452b-9b3e-c5a495736f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_chain = get_search_query_generation_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f6d66f1-e2b7-40ed-b495-be6f0d5e06a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RAG Chain\n"
     ]
    }
   ],
   "source": [
    "print('='*50)\n",
    "print('RAG Chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e607bdb-8960-4cf7-9569-7eedec5670c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    res = \"\"\n",
    "    # res = str(docs)\n",
    "    for doc in docs:\n",
    "        escaped_page_content = doc.page_content.replace(\"\\n\", \"\\\\n\")\n",
    "        res += \"<doc>\\n\"\n",
    "        res += f\"  <content>{escaped_page_content}</content>\\n\"\n",
    "        for m in doc.metadata:\n",
    "            res += f\"  <{m}>{doc.metadata[m]}</{m}>\\n\"\n",
    "        res += \"</doc>\\n\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2bdc0f0-037d-49fb-8154-41fe4a5325ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_chat_history(chat_history):\n",
    "    def format_single_chat_message(m):\n",
    "        if type(m) is HumanMessage:\n",
    "            return \"Human: \" + m.content\n",
    "        elif type(m) is AIMessage:\n",
    "            return \"Assistant: \" + m.content\n",
    "        elif type(m) is SystemMessage:\n",
    "            return \"System: \" + m.content\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role {m['role']}\")\n",
    "\n",
    "    return \"\\n\".join([format_single_chat_message(m) for m in chat_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a9922a6-a381-4826-9ffc-6b1f4d3162fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_index(file_name=\"Astha_Puri_Overall.pdf\", index_folder=\"index\"):\n",
    "    # load embeddings\n",
    "    from langchain.vectorstores import FAISS\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    search_index = FAISS.load_local(\n",
    "        folder_path=index_folder,\n",
    "        index_name=file_name + \".index\",\n",
    "        embeddings=OpenAIEmbeddings(),\n",
    "    )\n",
    "    return search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e1cc31-20d3-48fd-98dd-93190f98a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_chain(file_name=\"Astha_Puri_Overall.pdf\", index_folder=\"index\", retrieval_cb=None):\n",
    "    vectorstore = get_search_index(file_name, index_folder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    if retrieval_cb is None:\n",
    "        retrieval_cb = lambda x: x\n",
    "\n",
    "    def context_update_fn(q):\n",
    "        retrieval_cb([q])\n",
    "        return q\n",
    "\n",
    "    _inputs = RunnableMap(\n",
    "        standalone_question=RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    )\n",
    "    _context = {\n",
    "        \"context\": itemgetter(\"standalone_question\") | RunnablePassthrough(context_update_fn) | retriever | format_docs,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n",
    "    return conversational_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b067d87b-1f55-4b8b-a51f-1804429d440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided context, the individual seems to have a positive view of artificial intelligence. They mention collaborating to propagate responsible AI practices and empowering future AI professionals. Additionally, they have experience in implementing AI-driven solutions such as generative AI-driven POC for product review summaries and sentiment analysis. There is a focus on leveraging advanced NLP models to enhance customer satisfaction and provide informed purchasing choices. Overall, the individual appears to value the potential of AI in improving user experiences and achieving specific business goals. \\n\\nPage Number(s) Used: \\n- Page 0 (pdf/Astha_Puri_Overall.pdf)'\n"
     ]
    }
   ],
   "source": [
    "chain = get_rag_chain()\n",
    "print(chain.invoke({'input': 'artifical intelligence', 'chat_history': []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f2d36-eec2-4906-a2fa-1616eb178f40",
   "metadata": {},
   "source": [
    "### Chat - Question Generated Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cdc05ee-b656-4f39-b796-d04ed02216c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Question Generation Chain\n"
     ]
    }
   ],
   "source": [
    "print('='*50)\n",
    "print('Question Generation Chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0dfe581-a804-445f-93f9-5805b59d9728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What are the latest advancements in artificial intelligence technology?', '2. How is artificial intelligence being used in healthcare?', '3. What are the ethical implications of artificial intelligence?', '4. How can businesses leverage artificial intelligence for improved efficiency and productivity?']\n"
     ]
    }
   ],
   "source": [
    "print(question_generation_chain.invoke({'original_query': 'artifical intelligence'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dda08-a775-47de-8f4e-5e3714bfee62",
   "metadata": {},
   "source": [
    "### Chat - RAG Fusion Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55587282-6ff0-42c2-8920-2f51b07170f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "RAG Fusion Chain\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "print('RAG Fusion Chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d7d444b-3de9-43de-bb74-d720cdadf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG fusion chain\n",
    "# source1: https://youtu.be/GchC5WxeXGc?si=6i7J0rPZI7SNwFYZ\n",
    "# source2: https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1\n",
    "def reciprocal_rank_fusion(results, k=60):\n",
    "    from langchain.load import dumps, loads\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fdda440-297a-440e-b3aa-7c70373aca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_fusion_chain(file_name=\"Astha_Puri_Overall.pdf\", index_folder=\"index\", retrieval_cb=None):\n",
    "    vectorstore = get_search_index(file_name, index_folder)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    query_generation_chain = get_search_query_generation_chain()\n",
    "    _inputs = RunnableMap(\n",
    "        standalone_question=RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    )\n",
    "\n",
    "    if retrieval_cb is None:\n",
    "        retrieval_cb = lambda x: x\n",
    "\n",
    "    _context = {\n",
    "        \"context\":\n",
    "            RunnablePassthrough.assign(\n",
    "                original_query=lambda x: x[\"standalone_question\"]\n",
    "            )\n",
    "            | query_generation_chain\n",
    "            | retrieval_cb\n",
    "            | retriever.map()\n",
    "            | reciprocal_rank_fusion\n",
    "            | (lambda x: [item[0] for item in x])\n",
    "            | format_docs,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "    conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI()\n",
    "    return conversational_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9ebba34-6e76-4805-bbe8-a181ae124612",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_rag_fusion_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdf52324-ee12-49c1-a3f8-5a114aa9501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided context, the individual is skilled in various areas related to artificial intelligence such as Natural Language Processing (NLP), Neural Networks, Machine Learning, and Predictive Analytics. They also mention collaborating to propagate responsible AI practices and empowering future AI professionals. Additionally, they have worked on projects involving Generative AI-driven proof of concepts for product review summaries and sentiment analysis, as well as real-time search autocomplete using Recurrent Neural Networks (RNNs). These experiences suggest that the individual has a positive view of artificial intelligence and its potential applications for enhancing user experiences and customer satisfaction. \\n\\nSources: \\n- Astha_Puri_Overall.pdf, page 0'\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'input': 'artifical intelligence', 'chat_history': []}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
